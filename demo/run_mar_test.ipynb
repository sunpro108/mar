{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "y9RUzQUWhZEV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sunjinsheng/codebase/var_mar\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAR imports:\n",
    "import torch\n",
    "import numpy as np\n",
    "from models import mar\n",
    "from models.vae import AutoencoderKL\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from util import download\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from safetensors.torch import load_file\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pe8NYbWBsvLl"
   },
   "source": [
    "# 1. Load and download pre-trained MAR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x0EKkB_ssvLl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n",
      "Working with z of shape (1, 16, 16, 16) = 4096 dimensions.\n",
      "ckpt loaded\n"
     ]
    }
   ],
   "source": [
    "model_type = \"mar_huge\" #@param [\"mar_base\", \"mar_large\", \"mar_huge\"]\n",
    "num_sampling_steps_diffloss = 100 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
    "if model_type == \"mar_base\":\n",
    "  # download.download_pretrained_marb(overwrite=False)\n",
    "  diffloss_d = 6\n",
    "  diffloss_w = 1024\n",
    "elif model_type == \"mar_large\":\n",
    "  # download.download_pretrained_marl(overwrite=False)\n",
    "  diffloss_d = 8\n",
    "  diffloss_w = 1280\n",
    "elif model_type == \"mar_huge\":\n",
    "  # download.download_pretrained_marh(overwrite=False)\n",
    "  diffloss_d = 12\n",
    "  diffloss_w = 1536\n",
    "else:\n",
    "  raise NotImplementedError\n",
    "print('load model...')\n",
    "model = mar.__dict__[model_type](\n",
    "  buffer_size=64,\n",
    "  diffloss_d=diffloss_d,\n",
    "  diffloss_w=diffloss_w,\n",
    "  num_sampling_steps=str(num_sampling_steps_diffloss)\n",
    ").to(device)\n",
    "vae = AutoencoderKL(embed_dim=16, ch_mult=(1, 1, 2, 2, 4), ckpt_path=None).to(device)\n",
    "ckpt = load_file('checkpoints/mar-huge.safetensors', device='cuda')\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval() # important!\n",
    "ckpt = load_file('checkpoints/kl16.safetensors', device='cuda')\n",
    "vae.load_state_dict(ckpt)\n",
    "vae.eval()\n",
    "print('ckpt loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JTNyzNZKb9E"
   },
   "source": [
    "## Sample from Pre-trained MAR Models\n",
    "\n",
    "You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "-Hw7B5h4Kk4p",
    "outputId": "4948292b-e1d2-4ce9-de75-e58cdd74f0c4"
   },
   "outputs": [],
   "source": [
    "# Set user inputs:\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "num_ar_steps = 64 #@param {type:\"slider\", min:1, max:256, step:1}\n",
    "cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "cfg_schedule = \"constant\" #@param [\"linear\", \"constant\"]\n",
    "temperature = 1.0 #@param {type:\"slider\", min:0.9, max:1.1, step:0.01}\n",
    "class_labels = 207, 360, 388, 113, 355, 980, 323, 979 #@param {type:\"raw\"}\n",
    "samples_per_row = 4 #@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.amp.autocast('cuda'):\n",
    "  sampled_tokens = model.sample_tokens(\n",
    "      bsz=len(class_labels), num_iter=num_ar_steps,\n",
    "      cfg=cfg_scale, cfg_schedule=cfg_schedule,\n",
    "      labels=torch.Tensor(class_labels).long().cuda(),\n",
    "      temperature=temperature, progress=True)\n",
    "  sampled_images = vae.decode(sampled_tokens / 0.2325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and display images:\n",
    "# save_image(sampled_images, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "grid = make_grid(sampled_images, nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "samples = Image.fromarray(ndarr)\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = 207, \n",
    "bsz=len(class_labels)\n",
    "\n",
    "num_iter=num_ar_steps\n",
    "cfg=cfg_scale\n",
    "cfg_schedule=cfg_schedule\n",
    "labels=torch.Tensor(class_labels).long().cuda()\n",
    "temperature=temperature\n",
    "progress=True\n",
    "bsz, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_by_order(mask_len, order, bsz, seq_len):\n",
    "    masking = torch.zeros(bsz, seq_len).cuda()\n",
    "    masking = torch.scatter(masking, dim=-1, index=order[:, :mask_len.long()], src=torch.ones(bsz, seq_len).cuda())\n",
    "    return masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(range(num_iter))\n",
    "# if progress:\n",
    "#     indices = tqdm(indices)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_len, model.buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_tokens(model, bsz, num_iter=64, cfg=1.0, cfg_schedule=\"linear\", labels=None, temperature=1.0, progress=False):\n",
    "with torch.no_grad():\n",
    "    # init and sample generation orders\n",
    "    mask = torch.ones(bsz, model.seq_len).cuda()\n",
    "    tokens = torch.zeros(bsz, model.seq_len, model.token_embed_dim).cuda()\n",
    "    orders = model.sample_orders(bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 5\n",
    "list_masks = []\n",
    "list_tokens = []\n",
    "list_mask_ratios = []\n",
    "list_mask_lens = []\n",
    "list_mask_to_preds = []\n",
    "list_xs = []\n",
    "list_zs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256, 16]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape, tokens.shape, orders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # class embedding and CFG\n",
    "    if labels is not None:\n",
    "        class_embedding = model.class_emb(labels)\n",
    "    else:\n",
    "        class_embedding = model.fake_latent.repeat(bsz, 1)\n",
    "    if not cfg == 1.0:\n",
    "        class_embedding = torch.cat([class_embedding, model.fake_latent.repeat(bsz, 1)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "mae encoder\n",
      "torch.Size([2, 256, 16]) torch.Size([2, 256]) torch.Size([2, 1280])\n",
      "after z_proj x.shape: torch.Size([2, 256, 1280])\n",
      "after concat buffer x.shape: torch.Size([2, 320, 1280])\n",
      "after concat buffer mask_with_buffer.shape: torch.Size([2, 320])\n",
      "class_embedding shape: torch.Size([2, 1, 1280])\n",
      "before add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after ln x.shape: torch.Size([2, 320, 1280])\n",
      "after drop x.shape: torch.Size([2, 64, 1280])\n",
      "after block x.shape: torch.Size([2, 64, 1280])\n",
      "after norm x.shape: torch.Size([2, 64, 1280])\n",
      "mae encoder\n",
      "input x.shape: torch.Size([2, 64, 1280])\n",
      "embeded x.shape: torch.Size([2, 64, 1280])\n",
      "x_after_pad.shape: torch.Size([2, 320, 1280])\n",
      "add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "block x.shape: torch.Size([2, 320, 1280])\n",
      "decoder norm x.shape: torch.Size([2, 320, 1280])\n",
      "remove cls embed x.shape: torch.Size([2, 256, 1280])\n",
      "add diffusion pos embed x.shape: torch.Size([2, 256, 1280])\n",
      "1\n",
      "mae encoder\n",
      "torch.Size([2, 256, 16]) torch.Size([2, 256]) torch.Size([2, 1280])\n",
      "after z_proj x.shape: torch.Size([2, 256, 1280])\n",
      "after concat buffer x.shape: torch.Size([2, 320, 1280])\n",
      "after concat buffer mask_with_buffer.shape: torch.Size([2, 320])\n",
      "class_embedding shape: torch.Size([2, 1, 1280])\n",
      "before add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after ln x.shape: torch.Size([2, 320, 1280])\n",
      "after drop x.shape: torch.Size([2, 77, 1280])\n",
      "after block x.shape: torch.Size([2, 77, 1280])\n",
      "after norm x.shape: torch.Size([2, 77, 1280])\n",
      "mae encoder\n",
      "input x.shape: torch.Size([2, 77, 1280])\n",
      "embeded x.shape: torch.Size([2, 77, 1280])\n",
      "x_after_pad.shape: torch.Size([2, 320, 1280])\n",
      "add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "block x.shape: torch.Size([2, 320, 1280])\n",
      "decoder norm x.shape: torch.Size([2, 320, 1280])\n",
      "remove cls embed x.shape: torch.Size([2, 256, 1280])\n",
      "add diffusion pos embed x.shape: torch.Size([2, 256, 1280])\n",
      "2\n",
      "mae encoder\n",
      "torch.Size([2, 256, 16]) torch.Size([2, 256]) torch.Size([2, 1280])\n",
      "after z_proj x.shape: torch.Size([2, 256, 1280])\n",
      "after concat buffer x.shape: torch.Size([2, 320, 1280])\n",
      "after concat buffer mask_with_buffer.shape: torch.Size([2, 320])\n",
      "class_embedding shape: torch.Size([2, 1, 1280])\n",
      "before add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after ln x.shape: torch.Size([2, 320, 1280])\n",
      "after drop x.shape: torch.Size([2, 113, 1280])\n",
      "after block x.shape: torch.Size([2, 113, 1280])\n",
      "after norm x.shape: torch.Size([2, 113, 1280])\n",
      "mae encoder\n",
      "input x.shape: torch.Size([2, 113, 1280])\n",
      "embeded x.shape: torch.Size([2, 113, 1280])\n",
      "x_after_pad.shape: torch.Size([2, 320, 1280])\n",
      "add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "block x.shape: torch.Size([2, 320, 1280])\n",
      "decoder norm x.shape: torch.Size([2, 320, 1280])\n",
      "remove cls embed x.shape: torch.Size([2, 256, 1280])\n",
      "add diffusion pos embed x.shape: torch.Size([2, 256, 1280])\n",
      "3\n",
      "mae encoder\n",
      "torch.Size([2, 256, 16]) torch.Size([2, 256]) torch.Size([2, 1280])\n",
      "after z_proj x.shape: torch.Size([2, 256, 1280])\n",
      "after concat buffer x.shape: torch.Size([2, 320, 1280])\n",
      "after concat buffer mask_with_buffer.shape: torch.Size([2, 320])\n",
      "class_embedding shape: torch.Size([2, 1, 1280])\n",
      "before add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after ln x.shape: torch.Size([2, 320, 1280])\n",
      "after drop x.shape: torch.Size([2, 170, 1280])\n",
      "after block x.shape: torch.Size([2, 170, 1280])\n",
      "after norm x.shape: torch.Size([2, 170, 1280])\n",
      "mae encoder\n",
      "input x.shape: torch.Size([2, 170, 1280])\n",
      "embeded x.shape: torch.Size([2, 170, 1280])\n",
      "x_after_pad.shape: torch.Size([2, 320, 1280])\n",
      "add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "block x.shape: torch.Size([2, 320, 1280])\n",
      "decoder norm x.shape: torch.Size([2, 320, 1280])\n",
      "remove cls embed x.shape: torch.Size([2, 256, 1280])\n",
      "add diffusion pos embed x.shape: torch.Size([2, 256, 1280])\n",
      "4\n",
      "mae encoder\n",
      "torch.Size([2, 256, 16]) torch.Size([2, 256]) torch.Size([2, 1280])\n",
      "after z_proj x.shape: torch.Size([2, 256, 1280])\n",
      "after concat buffer x.shape: torch.Size([2, 320, 1280])\n",
      "after concat buffer mask_with_buffer.shape: torch.Size([2, 320])\n",
      "class_embedding shape: torch.Size([2, 1, 1280])\n",
      "before add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "after ln x.shape: torch.Size([2, 320, 1280])\n",
      "after drop x.shape: torch.Size([2, 241, 1280])\n",
      "after block x.shape: torch.Size([2, 241, 1280])\n",
      "after norm x.shape: torch.Size([2, 241, 1280])\n",
      "mae encoder\n",
      "input x.shape: torch.Size([2, 241, 1280])\n",
      "embeded x.shape: torch.Size([2, 241, 1280])\n",
      "x_after_pad.shape: torch.Size([2, 320, 1280])\n",
      "add pos embed x.shape: torch.Size([2, 320, 1280])\n",
      "block x.shape: torch.Size([2, 320, 1280])\n",
      "decoder norm x.shape: torch.Size([2, 320, 1280])\n",
      "remove cls embed x.shape: torch.Size([2, 256, 1280])\n",
      "add diffusion pos embed x.shape: torch.Size([2, 256, 1280])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # generate latents\n",
    "    for step in indices:\n",
    "        print(step)\n",
    "        cur_tokens = tokens.clone()\n",
    "\n",
    "        list_masks.append(mask.clone())\n",
    "\n",
    "        if not cfg == 1.0:\n",
    "            tokens = torch.cat([tokens, tokens], dim=0)\n",
    "            # class_embedding = torch.cat([class_embedding, model.fake_latent.repeat(bsz, 1)], dim=0)\n",
    "            mask = torch.cat([mask, mask], dim=0)\n",
    "\n",
    "        # mae encoder\n",
    "        print('mae encoder')\n",
    "        x = model.forward_mae_encoder(tokens, mask, class_embedding)\n",
    "        list_xs.append(x.clone())\n",
    "        # mae decoder\n",
    "        print('mae decoder')\n",
    "        z = model.forward_mae_decoder(x, mask)\n",
    "        list_zs.append(z.clone())\n",
    "\n",
    "        # mask ratio for the next round, following MaskGIT and MAGE.\n",
    "        mask_ratio = np.cos(np.pi / 2. * (step + 1) / num_iter)\n",
    "        # print(mask_ratio)\n",
    "        list_mask_ratios.append(mask_ratio)\n",
    "        mask_len = torch.Tensor([np.floor(model.seq_len * mask_ratio)]).cuda()\n",
    "        # masks out at least one for the next iteration\n",
    "        mask_len = torch.maximum(torch.Tensor([1]).cuda(),\n",
    "                                    torch.minimum(torch.sum(mask, dim=-1, keepdims=True) - 1, mask_len))\n",
    "        list_mask_lens.append(mask_len)\n",
    "\n",
    "        # get masking for next iteration and locations to be predicted in this iteration\n",
    "        mask_next = mask_by_order(mask_len[0], orders, bsz, model.seq_len)\n",
    "        if step >= num_iter - 1:\n",
    "            mask_to_pred = mask[:bsz].bool()\n",
    "        else:\n",
    "            mask_to_pred = torch.logical_xor(mask[:bsz].bool(), mask_next.bool())\n",
    "        mask = mask_next\n",
    "        if not cfg == 1.0:\n",
    "            mask_to_pred = torch.cat([mask_to_pred, mask_to_pred], dim=0)\n",
    "\n",
    "        # sample token latents for this step\n",
    "        z = z[mask_to_pred.nonzero(as_tuple=True)]\n",
    "        # cfg schedule follow Muse\n",
    "        if cfg_schedule == \"linear\":\n",
    "            cfg_iter = 1 + (cfg - 1) * (model.seq_len - mask_len[0]) / model.seq_len\n",
    "        elif cfg_schedule == \"constant\":\n",
    "            cfg_iter = cfg\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        sampled_token_latent = model.diffloss.sample(z, temperature, cfg_iter)\n",
    "        if not cfg == 1.0:\n",
    "            sampled_token_latent, _ = sampled_token_latent.chunk(2, dim=0)  # Remove null class samples\n",
    "            mask_to_pred, _ = mask_to_pred.chunk(2, dim=0)\n",
    "        list_mask_to_preds.append(mask_to_pred)\n",
    "        cur_tokens[mask_to_pred.nonzero(as_tuple=True)] = sampled_token_latent\n",
    "        tokens = cur_tokens.clone()\n",
    "        list_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list_masks[2]\n",
    "m[m.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder_pos_embed_learned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3,4,5)\n",
    "b = torch.ones(3,1,5)\n",
    "a[:, :2,] = b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{a:.4f}\" for a in list_mask_ratios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a[0].item() for a in list_mask_lens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = list_masks[4]\n",
    "mask = mask_next\n",
    "mask = mask.float()\n",
    "# mask = torch.ones(bsz, model.seq_len).cuda()\n",
    "mask_show = repeat(mask, \"n l -> n l d\", d=3)\n",
    "mask_show = rearrange(mask_show, 'n (h w) d -> n d h w', h = 16)\n",
    "mask_show = repeat(mask_show, 'n d h w -> n d p h q w', p = 16, q= 16)\n",
    "sampled_images = rearrange(mask_show, 'n d p h q w -> n (h w) d p q')\n",
    "# Save and display images:\n",
    "# save_image(sampled_images, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "grid = make_grid(sampled_images[0], nrow=int(16), normalize=True, value_range=(-1, 1))\n",
    "# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "samples = Image.fromarray(ndarr)\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = tokens\n",
    "mask = list_tokens[4]\n",
    "\n",
    "# mask = mask.float()\n",
    "# mask = torch.ones(bsz, model.seq_len).cuda()\n",
    "# mask_show = repeat(mask, \"n l -> n l d\", d=3)\n",
    "mask_show = mask[:,:,:3]\n",
    "mask_show = rearrange(mask_show, 'n (h w) d -> n d h w', h = 16)\n",
    "mask_show = repeat(mask_show, 'n d h w -> n d p h q w', p = 16, q= 16)\n",
    "sampled_images = rearrange(mask_show, 'n d p h q w -> n (h w) d p q')\n",
    "# Save and display images:\n",
    "# save_image(sampled_images, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "grid = make_grid(sampled_images[0], nrow=int(16), normalize=True, value_range=(-1, 1))\n",
    "# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "samples = Image.fromarray(ndarr)\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # unpatchify\n",
    "    tokens = model.unpatchify(tokens)\n",
    "    sampled_images = vae.decode(tokens / 0.2325)\n",
    "# return tokens\n",
    "# Save and display images:\n",
    "# save_image(sampled_images, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "grid = make_grid(sampled_images, nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "samples = Image.fromarray(ndarr)\n",
    "display(samples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
